{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Regression & Its Evaluation"
      ],
      "metadata": {
        "id": "uBeNPaIJZaDW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Simple Linear Regression?\n"
      ],
      "metadata": {
        "id": "YhJzvBrwXMxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Simple Linear Regression is a statistical technique used to understand and model the relationship between two variables.\n",
        "- Simple Linear Regression shows how one independent variable (X) affects one dependent variable (Y) by fitting a straight line through the data points.\n",
        "           \n",
        "            formula =  Y=a+bX"
      ],
      "metadata": {
        "id": "nKx9T6SqXQHY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What are the key assumptions of Simple Linear Regression?"
      ],
      "metadata": {
        "id": "ld8_E6yEXvKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The key assumptions of simple linear regression are linearity, independence of errors, homoscedasticity, normal distribution of errors, and absence of influential outliers."
      ],
      "metadata": {
        "id": "PxQ_K3QHXzSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is heteroscedasticity, and why is it important to address in regression models?"
      ],
      "metadata": {
        "id": "Qfxruj0SYBUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Heteroscedasticity refers to a situation in regression analysis where the variance of the error terms (residuals) is not constant across all levels of the independent variable.\n",
        "\n",
        "- Why is it Important to Address?\n",
        "\n",
        "1- Incorrect Standard Errors\n",
        "\n",
        "Leads to unreliable confidence intervals and p-values.\n",
        "\n",
        "2️- Invalid Hypothesis Tests\n",
        "\n",
        "t-tests and F-tests may give wrong conclusions.\n",
        "\n",
        "3️- Inefficient Estimates\n",
        "\n",
        "Coefficient estimates are less efficient (not minimum variance).\n",
        "\n",
        "4️-  Poor Model Reliability\n",
        "\n",
        "Predictions and inferences become less trustworthy."
      ],
      "metadata": {
        "id": "s2wP4QvBYJP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What is Multiple Linear Regression?"
      ],
      "metadata": {
        "id": "lsI_t03-YiK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Multiple Linear Regression (MLR) is a statistical technique used to model the relationship between one dependent variable and two or more independent variables.\n",
        "  \n",
        "         formula = Y=b0​+b1​X1​+b2​X2​+⋯+bn​Xn​"
      ],
      "metadata": {
        "id": "uMnTwETzYlk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is polynomial regression, and how does it differ from linear\n",
        "regression?"
      ],
      "metadata": {
        "id": "PFTHmosEYqSu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Polynomial Regression is a type of regression analysis where the relationship between the independent variable and the dependent variable is modeled as a polynomial function of the independent variable.\n",
        "\n",
        "              Formula = Y=b0​+b1​X+b2​X2+b3​X3+…"
      ],
      "metadata": {
        "id": "Il-ru8nZY83R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Given data\n",
        "X = np.array([1, 2, 3, 4, 5])\n",
        "Y = np.array([2.1, 4.3, 6.1, 7.9, 10.2])\n",
        "\n",
        "#Calculate slope (m) and intercept (c)\n",
        "x_mean = np.mean(X)\n",
        "y_mean = np.mean(Y)\n",
        "\n",
        "m = np.sum((X - x_mean) * (Y - y_mean)) / np.sum((X - x_mean) ** 2)\n",
        "c = y_mean - m * x_mean\n",
        "\n",
        "# Predicted values\n",
        "Y_pred = m * X + c\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, Y)\n",
        "plt.plot(X, Y_pred)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"Simple Linear Regression\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Slope (m):\", m)\n",
        "print(\"Intercept (c):\", c)"
      ],
      "metadata": {
        "id": "0av6kFbdsd4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. import pandas as pd\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
        "\n",
        "# Sample data\n",
        "data = {\n",
        "    \"Area\": [1200, 1500, 1800, 2000],\n",
        "    \"Rooms\": [2, 3, 3, 4],\n",
        "    \"Price\": [250000, 300000, 320000, 370000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Independent variables\n",
        "X = df[[\"Area\", \"Rooms\"]]\n",
        "y = df[\"Price\"]\n",
        "\n",
        "# Add constant\n",
        "X_const = sm.add_constant(X)\n",
        "\n",
        "# Fit Multiple Linear Regression model\n",
        "model = sm.OLS(y, X_const).fit()\n",
        "print(model.summary())\n",
        "\n",
        "# Calculate VIF\n",
        "vif = pd.DataFrame()\n",
        "vif[\"Feature\"] = X_const.columns\n",
        "vif[\"VIF\"] = [\n",
        "    variance_inflation_factor(X_const.values, i)\n",
        "    for i in range(X_const.shape[1])\n",
        "]\n",
        "\n",
        "print(\"\\nVariance Inflation Factor (VIF):\")\n",
        "print(vif)"
      ],
      "metadata": {
        "id": "i8d3Lrxss8tD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Given data\n",
        "X = np.array([1, 2, 3, 4, 5])\n",
        "Y = np.array([2.2, 4.8, 7.5, 11.2, 14.7])\n",
        "\n",
        "# Fit 2nd-degree polynomial\n",
        "coefficients = np.polyfit(X, Y, 2)\n",
        "poly_model = np.poly1d(coefficients)\n",
        "\n",
        "# Generate smooth curve\n",
        "X_curve = np.linspace(min(X), max(X), 100)\n",
        "Y_curve = poly_model(X_curve)\n",
        "\n",
        "# Plot\n",
        "plt.scatter(X, Y)\n",
        "plt.plot(X_curve, Y_curve)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Y\")\n",
        "plt.title(\"2nd Degree Polynomial Regression\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Polynomial Coefficients:\", coefficients)"
      ],
      "metadata": {
        "id": "y4SzSuYctAuv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Given data\n",
        "X = np.array([10, 20, 30, 40, 50])\n",
        "Y = np.array([15, 35, 40, 50, 65])\n",
        "\n",
        "# Fit simple linear regression (y = mx + c)\n",
        "x_mean = np.mean(X)\n",
        "y_mean = np.mean(Y)\n",
        "\n",
        "m = np.sum((X - x_mean) * (Y - y_mean)) / np.sum((X - x_mean) ** 2)\n",
        "c = y_mean - m * x_mean\n",
        "\n",
        "# Predictions\n",
        "Y_pred = m * X + c\n",
        "\n",
        "# Residuals\n",
        "residuals = Y - Y_pred\n",
        "\n",
        "# Residuals plot\n",
        "plt.scatter(X, residuals)\n",
        "plt.axhline(0)\n",
        "plt.xlabel(\"X\")\n",
        "plt.ylabel(\"Residuals\")\n",
        "plt.title(\"Residuals Plot\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2uIrC6NqtGqw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you are a data scientist working for a real estate company. You\n",
        "need to predict house prices using features like area, number of rooms, and location.\n",
        "However, you detect heteroscedasticity and multicollinearity in your regression\n",
        "model. Explain the steps you would take to address these issues and ensure a robust\n",
        "model.\n",
        "- As a data scientist predicting **house prices** using features like **area, rooms, and location**, the regression model shows:\n",
        "\n",
        "-  Heteroscedasticity (non-constant error variance)\n",
        "- Multicollinearity (high correlation among predictors)\n",
        "\n",
        "Both issues can make the model unreliable and unstable.\n",
        "1. Handling Heteroscedasticity\n",
        "- Plot **residuals vs predicted values**\n",
        "- Perform **Breusch–Pagan or White test**\n",
        " Solutions\n",
        "\n",
        "1. **Transform the target variable**\n",
        "\n",
        "   * Apply **log, square root, or Box-Cox transformation** to house prices\n",
        "   * Example: `log(Price)`\n",
        "\n",
        "2. **Use Weighted Least Squares (WLS)**\n",
        "\n",
        "   * Assign lower weights to observations with high variance\n",
        "\n",
        "3. **Robust Standard Errors**\n",
        "\n",
        "   * Use **heteroscedasticity-robust (HC) standard errors** to get reliable p-values\n",
        "\n",
        "4. **Feature engineering**\n",
        "\n",
        "   * Add missing variables (e.g., neighborhood quality, age of house)\n",
        "\n",
        "\n",
        "\n",
        "2.  Handling Multicollinearity\n",
        "\n",
        "* Compute **Variance Inflation Factor (VIF)**\n",
        "* Check **correlation matrix**\n",
        "\n",
        "Solutions\n",
        "\n",
        "1. **Remove or combine correlated features**\n",
        "\n",
        "   * Example: Combine `Area` and `Rooms` into **price per square foot**\n",
        "\n",
        "2. **Feature selection**\n",
        "\n",
        "   * Use **stepwise regression**, **Lasso**, or **Ridge regression**\n",
        "\n",
        "3. **Regularization**\n",
        "\n",
        "   * **Ridge Regression** → reduces coefficient variance\n",
        "   * **Lasso Regression** → removes less important features\n",
        "\n",
        "4. **Principal Component Analysis (PCA)**\n",
        "\n",
        "   * Convert correlated features into independent components\n",
        "\n",
        "3.  Ensuring a Robust Model\n",
        "\n",
        "- Re-train the model after corrections\n",
        "- Validate using **cross-validation**\n",
        "- Check assumptions again\n",
        "- Evaluate performance using **RMSE and R²**\n",
        "-  Interpret coefficients carefully"
      ],
      "metadata": {
        "id": "aGN7s_qJtLOD"
      }
    }
  ]
}